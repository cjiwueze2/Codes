{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c909fa92-4bfc-41ea-bc9f-a6e09e44b79f",
   "metadata": {},
   "source": [
    "## Fruit Classification\n",
    "\n",
    "#### A model that can identify the type of date from external features such as colour, length, diameter and shape factors which have been determined by a computer vision model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "381b02b5-7c2c-41a8-a74e-001023aed22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c257d194-7051-4d0c-b7a5-b89241962436",
   "metadata": {},
   "source": [
    "### Load the Data Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "fe64d713-9016-403d-b594-c0c5345a5864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AREA</th>\n",
       "      <th>PERIMETER</th>\n",
       "      <th>MAJOR_AXIS</th>\n",
       "      <th>MINOR_AXIS</th>\n",
       "      <th>ECCENTRICITY</th>\n",
       "      <th>EQDIASQ</th>\n",
       "      <th>SOLIDITY</th>\n",
       "      <th>CONVEX_AREA</th>\n",
       "      <th>EXTENT</th>\n",
       "      <th>ASPECT_RATIO</th>\n",
       "      <th>...</th>\n",
       "      <th>KurtosisRR</th>\n",
       "      <th>KurtosisRG</th>\n",
       "      <th>KurtosisRB</th>\n",
       "      <th>EntropyRR</th>\n",
       "      <th>EntropyRG</th>\n",
       "      <th>EntropyRB</th>\n",
       "      <th>ALLdaub4RR</th>\n",
       "      <th>ALLdaub4RG</th>\n",
       "      <th>ALLdaub4RB</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>422163</td>\n",
       "      <td>2378.908</td>\n",
       "      <td>837.8484</td>\n",
       "      <td>645.6693</td>\n",
       "      <td>0.6373</td>\n",
       "      <td>733.1539</td>\n",
       "      <td>0.9947</td>\n",
       "      <td>424428</td>\n",
       "      <td>0.7831</td>\n",
       "      <td>1.2976</td>\n",
       "      <td>...</td>\n",
       "      <td>3.2370</td>\n",
       "      <td>2.9574</td>\n",
       "      <td>4.2287</td>\n",
       "      <td>-5.919126e+10</td>\n",
       "      <td>-50714214400</td>\n",
       "      <td>-39922372608</td>\n",
       "      <td>58.7255</td>\n",
       "      <td>54.9554</td>\n",
       "      <td>47.8400</td>\n",
       "      <td>BERHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>338136</td>\n",
       "      <td>2085.144</td>\n",
       "      <td>723.8198</td>\n",
       "      <td>595.2073</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>656.1464</td>\n",
       "      <td>0.9974</td>\n",
       "      <td>339014</td>\n",
       "      <td>0.7795</td>\n",
       "      <td>1.2161</td>\n",
       "      <td>...</td>\n",
       "      <td>2.6228</td>\n",
       "      <td>2.6350</td>\n",
       "      <td>3.1704</td>\n",
       "      <td>-3.423307e+10</td>\n",
       "      <td>-37462601728</td>\n",
       "      <td>-31477794816</td>\n",
       "      <td>50.0259</td>\n",
       "      <td>52.8168</td>\n",
       "      <td>47.8315</td>\n",
       "      <td>BERHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>526843</td>\n",
       "      <td>2647.394</td>\n",
       "      <td>940.7379</td>\n",
       "      <td>715.3638</td>\n",
       "      <td>0.6494</td>\n",
       "      <td>819.0222</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>528876</td>\n",
       "      <td>0.7657</td>\n",
       "      <td>1.3150</td>\n",
       "      <td>...</td>\n",
       "      <td>3.7516</td>\n",
       "      <td>3.8611</td>\n",
       "      <td>4.7192</td>\n",
       "      <td>-9.394835e+10</td>\n",
       "      <td>-74738221056</td>\n",
       "      <td>-60311207936</td>\n",
       "      <td>65.4772</td>\n",
       "      <td>59.2860</td>\n",
       "      <td>51.9378</td>\n",
       "      <td>BERHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>416063</td>\n",
       "      <td>2351.210</td>\n",
       "      <td>827.9804</td>\n",
       "      <td>645.2988</td>\n",
       "      <td>0.6266</td>\n",
       "      <td>727.8378</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>418255</td>\n",
       "      <td>0.7759</td>\n",
       "      <td>1.2831</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0401</td>\n",
       "      <td>8.6136</td>\n",
       "      <td>8.2618</td>\n",
       "      <td>-3.207431e+10</td>\n",
       "      <td>-32060925952</td>\n",
       "      <td>-29575010304</td>\n",
       "      <td>43.3900</td>\n",
       "      <td>44.1259</td>\n",
       "      <td>41.1882</td>\n",
       "      <td>BERHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>347562</td>\n",
       "      <td>2160.354</td>\n",
       "      <td>763.9877</td>\n",
       "      <td>582.8359</td>\n",
       "      <td>0.6465</td>\n",
       "      <td>665.2291</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>350797</td>\n",
       "      <td>0.7569</td>\n",
       "      <td>1.3108</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7016</td>\n",
       "      <td>2.9761</td>\n",
       "      <td>4.4146</td>\n",
       "      <td>-3.998097e+10</td>\n",
       "      <td>-35980042240</td>\n",
       "      <td>-25593278464</td>\n",
       "      <td>52.7743</td>\n",
       "      <td>50.9080</td>\n",
       "      <td>42.6666</td>\n",
       "      <td>BERHI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AREA  PERIMETER  MAJOR_AXIS  MINOR_AXIS  ECCENTRICITY   EQDIASQ  \\\n",
       "0  422163   2378.908    837.8484    645.6693        0.6373  733.1539   \n",
       "1  338136   2085.144    723.8198    595.2073        0.5690  656.1464   \n",
       "2  526843   2647.394    940.7379    715.3638        0.6494  819.0222   \n",
       "3  416063   2351.210    827.9804    645.2988        0.6266  727.8378   \n",
       "4  347562   2160.354    763.9877    582.8359        0.6465  665.2291   \n",
       "\n",
       "   SOLIDITY  CONVEX_AREA  EXTENT  ASPECT_RATIO  ...  KurtosisRR  KurtosisRG  \\\n",
       "0    0.9947       424428  0.7831        1.2976  ...      3.2370      2.9574   \n",
       "1    0.9974       339014  0.7795        1.2161  ...      2.6228      2.6350   \n",
       "2    0.9962       528876  0.7657        1.3150  ...      3.7516      3.8611   \n",
       "3    0.9948       418255  0.7759        1.2831  ...      5.0401      8.6136   \n",
       "4    0.9908       350797  0.7569        1.3108  ...      2.7016      2.9761   \n",
       "\n",
       "   KurtosisRB     EntropyRR    EntropyRG    EntropyRB  ALLdaub4RR  ALLdaub4RG  \\\n",
       "0      4.2287 -5.919126e+10 -50714214400 -39922372608     58.7255     54.9554   \n",
       "1      3.1704 -3.423307e+10 -37462601728 -31477794816     50.0259     52.8168   \n",
       "2      4.7192 -9.394835e+10 -74738221056 -60311207936     65.4772     59.2860   \n",
       "3      8.2618 -3.207431e+10 -32060925952 -29575010304     43.3900     44.1259   \n",
       "4      4.4146 -3.998097e+10 -35980042240 -25593278464     52.7743     50.9080   \n",
       "\n",
       "   ALLdaub4RB  Class  \n",
       "0     47.8400  BERHI  \n",
       "1     47.8315  BERHI  \n",
       "2     51.9378  BERHI  \n",
       "3     41.1882  BERHI  \n",
       "4     42.6666  BERHI  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('Date_Fruit_Datasets.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "cba3cbd9-faaf-4ee7-a020-9d5541f849c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AREA</th>\n",
       "      <th>PERIMETER</th>\n",
       "      <th>MAJOR_AXIS</th>\n",
       "      <th>MINOR_AXIS</th>\n",
       "      <th>ECCENTRICITY</th>\n",
       "      <th>EQDIASQ</th>\n",
       "      <th>SOLIDITY</th>\n",
       "      <th>CONVEX_AREA</th>\n",
       "      <th>EXTENT</th>\n",
       "      <th>ASPECT_RATIO</th>\n",
       "      <th>...</th>\n",
       "      <th>SkewRB</th>\n",
       "      <th>KurtosisRR</th>\n",
       "      <th>KurtosisRG</th>\n",
       "      <th>KurtosisRB</th>\n",
       "      <th>EntropyRR</th>\n",
       "      <th>EntropyRG</th>\n",
       "      <th>EntropyRB</th>\n",
       "      <th>ALLdaub4RR</th>\n",
       "      <th>ALLdaub4RG</th>\n",
       "      <th>ALLdaub4RB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>8.980000e+02</td>\n",
       "      <td>8.980000e+02</td>\n",
       "      <td>8.980000e+02</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>898.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>298295.207127</td>\n",
       "      <td>2057.660953</td>\n",
       "      <td>750.811994</td>\n",
       "      <td>495.872785</td>\n",
       "      <td>0.737468</td>\n",
       "      <td>604.577938</td>\n",
       "      <td>0.981840</td>\n",
       "      <td>303845.592428</td>\n",
       "      <td>0.736267</td>\n",
       "      <td>2.131102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250518</td>\n",
       "      <td>4.247845</td>\n",
       "      <td>5.110894</td>\n",
       "      <td>3.780928</td>\n",
       "      <td>-3.185021e+10</td>\n",
       "      <td>-2.901860e+10</td>\n",
       "      <td>-2.771876e+10</td>\n",
       "      <td>50.082888</td>\n",
       "      <td>48.805681</td>\n",
       "      <td>48.098393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>107245.205337</td>\n",
       "      <td>410.012459</td>\n",
       "      <td>144.059326</td>\n",
       "      <td>114.268917</td>\n",
       "      <td>0.088727</td>\n",
       "      <td>119.593888</td>\n",
       "      <td>0.018157</td>\n",
       "      <td>108815.656947</td>\n",
       "      <td>0.053745</td>\n",
       "      <td>17.820778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.632918</td>\n",
       "      <td>2.892357</td>\n",
       "      <td>3.745463</td>\n",
       "      <td>2.049831</td>\n",
       "      <td>2.037241e+10</td>\n",
       "      <td>1.712952e+10</td>\n",
       "      <td>1.484137e+10</td>\n",
       "      <td>16.063125</td>\n",
       "      <td>14.125911</td>\n",
       "      <td>10.813862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1987.000000</td>\n",
       "      <td>911.828000</td>\n",
       "      <td>336.722700</td>\n",
       "      <td>2.283200</td>\n",
       "      <td>0.344800</td>\n",
       "      <td>50.298400</td>\n",
       "      <td>0.836600</td>\n",
       "      <td>2257.000000</td>\n",
       "      <td>0.512300</td>\n",
       "      <td>1.065300</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.029100</td>\n",
       "      <td>1.708200</td>\n",
       "      <td>1.607600</td>\n",
       "      <td>1.767200</td>\n",
       "      <td>-1.091220e+11</td>\n",
       "      <td>-9.261697e+10</td>\n",
       "      <td>-8.747177e+10</td>\n",
       "      <td>15.191100</td>\n",
       "      <td>20.524700</td>\n",
       "      <td>22.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>206948.000000</td>\n",
       "      <td>1726.091500</td>\n",
       "      <td>641.068650</td>\n",
       "      <td>404.684375</td>\n",
       "      <td>0.685625</td>\n",
       "      <td>513.317075</td>\n",
       "      <td>0.978825</td>\n",
       "      <td>210022.750000</td>\n",
       "      <td>0.705875</td>\n",
       "      <td>1.373725</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196950</td>\n",
       "      <td>2.536625</td>\n",
       "      <td>2.508850</td>\n",
       "      <td>2.577275</td>\n",
       "      <td>-4.429444e+10</td>\n",
       "      <td>-3.894638e+10</td>\n",
       "      <td>-3.564534e+10</td>\n",
       "      <td>38.224425</td>\n",
       "      <td>38.654525</td>\n",
       "      <td>39.250725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>319833.000000</td>\n",
       "      <td>2196.345450</td>\n",
       "      <td>791.363400</td>\n",
       "      <td>495.054850</td>\n",
       "      <td>0.754700</td>\n",
       "      <td>638.140950</td>\n",
       "      <td>0.987300</td>\n",
       "      <td>327207.000000</td>\n",
       "      <td>0.746950</td>\n",
       "      <td>1.524150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135550</td>\n",
       "      <td>3.069800</td>\n",
       "      <td>3.127800</td>\n",
       "      <td>3.080700</td>\n",
       "      <td>-2.826156e+10</td>\n",
       "      <td>-2.620990e+10</td>\n",
       "      <td>-2.392928e+10</td>\n",
       "      <td>53.841300</td>\n",
       "      <td>50.337800</td>\n",
       "      <td>49.614100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>382573.000000</td>\n",
       "      <td>2389.716575</td>\n",
       "      <td>858.633750</td>\n",
       "      <td>589.031700</td>\n",
       "      <td>0.802150</td>\n",
       "      <td>697.930525</td>\n",
       "      <td>0.991800</td>\n",
       "      <td>388804.000000</td>\n",
       "      <td>0.775850</td>\n",
       "      <td>1.674750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.593950</td>\n",
       "      <td>4.449850</td>\n",
       "      <td>7.320400</td>\n",
       "      <td>4.283125</td>\n",
       "      <td>-1.460482e+10</td>\n",
       "      <td>-1.433105e+10</td>\n",
       "      <td>-1.660367e+10</td>\n",
       "      <td>63.063350</td>\n",
       "      <td>59.573600</td>\n",
       "      <td>56.666675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>546063.000000</td>\n",
       "      <td>2811.997100</td>\n",
       "      <td>1222.723000</td>\n",
       "      <td>766.453600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>833.827900</td>\n",
       "      <td>0.997400</td>\n",
       "      <td>552598.000000</td>\n",
       "      <td>0.856200</td>\n",
       "      <td>535.525700</td>\n",
       "      <td>...</td>\n",
       "      <td>3.092300</td>\n",
       "      <td>26.171100</td>\n",
       "      <td>26.736700</td>\n",
       "      <td>32.249500</td>\n",
       "      <td>-1.627316e+08</td>\n",
       "      <td>-5.627727e+08</td>\n",
       "      <td>-4.370435e+08</td>\n",
       "      <td>79.828900</td>\n",
       "      <td>83.064900</td>\n",
       "      <td>74.104600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                AREA    PERIMETER   MAJOR_AXIS  MINOR_AXIS  ECCENTRICITY  \\\n",
       "count     898.000000   898.000000   898.000000  898.000000    898.000000   \n",
       "mean   298295.207127  2057.660953   750.811994  495.872785      0.737468   \n",
       "std    107245.205337   410.012459   144.059326  114.268917      0.088727   \n",
       "min      1987.000000   911.828000   336.722700    2.283200      0.344800   \n",
       "25%    206948.000000  1726.091500   641.068650  404.684375      0.685625   \n",
       "50%    319833.000000  2196.345450   791.363400  495.054850      0.754700   \n",
       "75%    382573.000000  2389.716575   858.633750  589.031700      0.802150   \n",
       "max    546063.000000  2811.997100  1222.723000  766.453600      1.000000   \n",
       "\n",
       "          EQDIASQ    SOLIDITY    CONVEX_AREA      EXTENT  ASPECT_RATIO  ...  \\\n",
       "count  898.000000  898.000000     898.000000  898.000000    898.000000  ...   \n",
       "mean   604.577938    0.981840  303845.592428    0.736267      2.131102  ...   \n",
       "std    119.593888    0.018157  108815.656947    0.053745     17.820778  ...   \n",
       "min     50.298400    0.836600    2257.000000    0.512300      1.065300  ...   \n",
       "25%    513.317075    0.978825  210022.750000    0.705875      1.373725  ...   \n",
       "50%    638.140950    0.987300  327207.000000    0.746950      1.524150  ...   \n",
       "75%    697.930525    0.991800  388804.000000    0.775850      1.674750  ...   \n",
       "max    833.827900    0.997400  552598.000000    0.856200    535.525700  ...   \n",
       "\n",
       "           SkewRB  KurtosisRR  KurtosisRG  KurtosisRB     EntropyRR  \\\n",
       "count  898.000000  898.000000  898.000000  898.000000  8.980000e+02   \n",
       "mean     0.250518    4.247845    5.110894    3.780928 -3.185021e+10   \n",
       "std      0.632918    2.892357    3.745463    2.049831  2.037241e+10   \n",
       "min     -1.029100    1.708200    1.607600    1.767200 -1.091220e+11   \n",
       "25%     -0.196950    2.536625    2.508850    2.577275 -4.429444e+10   \n",
       "50%      0.135550    3.069800    3.127800    3.080700 -2.826156e+10   \n",
       "75%      0.593950    4.449850    7.320400    4.283125 -1.460482e+10   \n",
       "max      3.092300   26.171100   26.736700   32.249500 -1.627316e+08   \n",
       "\n",
       "          EntropyRG     EntropyRB  ALLdaub4RR  ALLdaub4RG  ALLdaub4RB  \n",
       "count  8.980000e+02  8.980000e+02  898.000000  898.000000  898.000000  \n",
       "mean  -2.901860e+10 -2.771876e+10   50.082888   48.805681   48.098393  \n",
       "std    1.712952e+10  1.484137e+10   16.063125   14.125911   10.813862  \n",
       "min   -9.261697e+10 -8.747177e+10   15.191100   20.524700   22.130000  \n",
       "25%   -3.894638e+10 -3.564534e+10   38.224425   38.654525   39.250725  \n",
       "50%   -2.620990e+10 -2.392928e+10   53.841300   50.337800   49.614100  \n",
       "75%   -1.433105e+10 -1.660367e+10   63.063350   59.573600   56.666675  \n",
       "max   -5.627727e+08 -4.370435e+08   79.828900   83.064900   74.104600  \n",
       "\n",
       "[8 rows x 34 columns]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a836aff8-7741-4f11-8e3e-3a7af03a94c5",
   "metadata": {},
   "source": [
    "### Explanation          \n",
    "Area from the descriptive statistics indicate the possibility of outliers having a minimum value of 1987 and the average of 298,295 and the quarters have values from 206,945 to 546063.  \n",
    "\n",
    "The same applies to CONVEX_AREA. ASPECT_RATIO (min and max of 1.0 and 535.5 respectively)\n",
    "\n",
    "This dataset describes various features related to shape and texture, such as area, perimeter, major and minor axes, eccentricity, and various statistical measures (e.g., skewness, kurtosis, entropy). Here's a summary of key observations:\n",
    "\n",
    "1. Central Tendency: The mean area is approximately 298,295, while the mean perimeter is around 2,057, indicating large-sized objects or regions.\n",
    "2. Spread: There is a high standard deviation in most features, especially in metrics like AREA, PERIMETER, and MAJOR_AXIS, suggesting a wide variation.\n",
    "3. Outliers: The wide range between the minimum and maximum values in features like AREA and PERIMETER indicates potential outliers or varied shapes. For instance, the AREA ranges from 1,987 to 546,063.\n",
    "4. Entropy Measures: The entropy values (EntropyRR, EntropyRG, EntropyRB) are negative with large magnitudes, which could indicate low randomness or structured patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "3c3f5471-8bba-42bc-85e0-3082dae806a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 898 entries, 0 to 897\n",
      "Data columns (total 35 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   AREA           898 non-null    int64  \n",
      " 1   PERIMETER      898 non-null    float64\n",
      " 2   MAJOR_AXIS     898 non-null    float64\n",
      " 3   MINOR_AXIS     898 non-null    float64\n",
      " 4   ECCENTRICITY   898 non-null    float64\n",
      " 5   EQDIASQ        898 non-null    float64\n",
      " 6   SOLIDITY       898 non-null    float64\n",
      " 7   CONVEX_AREA    898 non-null    int64  \n",
      " 8   EXTENT         898 non-null    float64\n",
      " 9   ASPECT_RATIO   898 non-null    float64\n",
      " 10  ROUNDNESS      898 non-null    float64\n",
      " 11  COMPACTNESS    898 non-null    float64\n",
      " 12  SHAPEFACTOR_1  898 non-null    float64\n",
      " 13  SHAPEFACTOR_2  898 non-null    float64\n",
      " 14  SHAPEFACTOR_3  898 non-null    float64\n",
      " 15  SHAPEFACTOR_4  898 non-null    float64\n",
      " 16  MeanRR         898 non-null    float64\n",
      " 17  MeanRG         898 non-null    float64\n",
      " 18  MeanRB         898 non-null    float64\n",
      " 19  StdDevRR       898 non-null    float64\n",
      " 20  StdDevRG       898 non-null    float64\n",
      " 21  StdDevRB       898 non-null    float64\n",
      " 22  SkewRR         898 non-null    float64\n",
      " 23  SkewRG         898 non-null    float64\n",
      " 24  SkewRB         898 non-null    float64\n",
      " 25  KurtosisRR     898 non-null    float64\n",
      " 26  KurtosisRG     898 non-null    float64\n",
      " 27  KurtosisRB     898 non-null    float64\n",
      " 28  EntropyRR      898 non-null    float64\n",
      " 29  EntropyRG      898 non-null    int64  \n",
      " 30  EntropyRB      898 non-null    int64  \n",
      " 31  ALLdaub4RR     898 non-null    float64\n",
      " 32  ALLdaub4RG     898 non-null    float64\n",
      " 33  ALLdaub4RB     898 non-null    float64\n",
      " 34  Class          898 non-null    object \n",
      "dtypes: float64(30), int64(4), object(1)\n",
      "memory usage: 245.7+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5c0121-b4d1-4f25-bd9d-85da73f26d89",
   "metadata": {},
   "source": [
    "Explanation\n",
    "\n",
    "There records indicate that there are no null values and the datatypes for the records are ok. The Class indicates that its an object having string or complex data types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "5da3f555-a387-4526-85e6-3a87169801a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AREA</th>\n",
       "      <th>PERIMETER</th>\n",
       "      <th>MAJOR_AXIS</th>\n",
       "      <th>MINOR_AXIS</th>\n",
       "      <th>ECCENTRICITY</th>\n",
       "      <th>EQDIASQ</th>\n",
       "      <th>SOLIDITY</th>\n",
       "      <th>CONVEX_AREA</th>\n",
       "      <th>EXTENT</th>\n",
       "      <th>ASPECT_RATIO</th>\n",
       "      <th>...</th>\n",
       "      <th>KurtosisRR</th>\n",
       "      <th>KurtosisRG</th>\n",
       "      <th>KurtosisRB</th>\n",
       "      <th>EntropyRR</th>\n",
       "      <th>EntropyRG</th>\n",
       "      <th>EntropyRB</th>\n",
       "      <th>ALLdaub4RR</th>\n",
       "      <th>ALLdaub4RG</th>\n",
       "      <th>ALLdaub4RB</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [AREA, PERIMETER, MAJOR_AXIS, MINOR_AXIS, ECCENTRICITY, EQDIASQ, SOLIDITY, CONVEX_AREA, EXTENT, ASPECT_RATIO, ROUNDNESS, COMPACTNESS, SHAPEFACTOR_1, SHAPEFACTOR_2, SHAPEFACTOR_3, SHAPEFACTOR_4, MeanRR, MeanRG, MeanRB, StdDevRR, StdDevRG, StdDevRB, SkewRR, SkewRG, SkewRB, KurtosisRR, KurtosisRG, KurtosisRB, EntropyRR, EntropyRG, EntropyRB, ALLdaub4RR, ALLdaub4RG, ALLdaub4RB, Class]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 35 columns]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates = dataset[dataset.duplicated()]\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f598fb6-4266-4ac3-888a-c27bcd115eb7",
   "metadata": {},
   "source": [
    "### The Dataset has no duplicated values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "6052efb2-2312-4e46-8636-ff831d3f6232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Class\n",
       "DOKOL     204\n",
       "SAFAVI    199\n",
       "ROTANA    166\n",
       "DEGLET     98\n",
       "SOGAY      94\n",
       "IRAQI      72\n",
       "BERHI      65\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = dataset['Class'].value_counts()\n",
    "\n",
    "num_class = len(counts)\n",
    "print('Number of Classes:',  num_class)\n",
    "counts #.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b4608351-a83d-4e86-a878-48386fdb25c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a bar plot\n",
    "sns.barplot(x=counts.index, y=counts.values)\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Number of Elements in Each Class')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b502dbfc-7c44-4256-b90d-31269c0d73dd",
   "metadata": {},
   "source": [
    "### Observation\n",
    "The dataset contains five distinct classes of data with varying frequencies. Here's a brief description of each class and the overall distribution:\n",
    "\n",
    "#### Class Distribution:\n",
    "\n",
    "##### DOKOL: 204 instances\n",
    "##### SAFAVI: 199 instances\n",
    "##### ROTANA: 166 instances\n",
    "##### DEGLET: 98 instances\n",
    "##### SOGAY: 94 instances\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "The class distribution is somewhat imbalanced, with the class \"DOKOL\" having the highest number of instances (204) and \"SOGAY\" having the lowest (94).\n",
    "\n",
    "\"DOKOL\" and \"SAFAVI\" are the most represented classes, together comprising a significant portion of the dataset.\n",
    "\n",
    "\"ROTANA\" also has a relatively high count but is less frequent than the top two classes.\n",
    "\n",
    "\"DEGLET\" and \"SOGAY\" have notably fewer instances, potentially leading to challenges in training models that perform well across all classes.\n",
    "\n",
    "Implications:\n",
    "\n",
    "This imbalance could affect the performance of machine learning models, potentially biasing them toward the more frequent classes (\"DOKOL\" and \"SAFAVI\").\n",
    "\n",
    "Special techniques like resampling, using class weights, or data augmentation may be needed to address this imbalance for better generalization across all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b2b022ef-8e69-43a1-9f2c-46f261eff09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    BERHI\n",
       "1    BERHI\n",
       "2    BERHI\n",
       "3    BERHI\n",
       "4    BERHI\n",
       "Name: Class, dtype: object"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = dataset['Class']\n",
    "label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "eb82817b-b9e9-4f1c-8d94-9cbc5bd950b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AREA</th>\n",
       "      <th>PERIMETER</th>\n",
       "      <th>MAJOR_AXIS</th>\n",
       "      <th>MINOR_AXIS</th>\n",
       "      <th>ECCENTRICITY</th>\n",
       "      <th>EQDIASQ</th>\n",
       "      <th>SOLIDITY</th>\n",
       "      <th>CONVEX_AREA</th>\n",
       "      <th>EXTENT</th>\n",
       "      <th>ASPECT_RATIO</th>\n",
       "      <th>...</th>\n",
       "      <th>SkewRB</th>\n",
       "      <th>KurtosisRR</th>\n",
       "      <th>KurtosisRG</th>\n",
       "      <th>KurtosisRB</th>\n",
       "      <th>EntropyRR</th>\n",
       "      <th>EntropyRG</th>\n",
       "      <th>EntropyRB</th>\n",
       "      <th>ALLdaub4RR</th>\n",
       "      <th>ALLdaub4RG</th>\n",
       "      <th>ALLdaub4RB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>422163</td>\n",
       "      <td>2378.908</td>\n",
       "      <td>837.8484</td>\n",
       "      <td>645.6693</td>\n",
       "      <td>0.6373</td>\n",
       "      <td>733.1539</td>\n",
       "      <td>0.9947</td>\n",
       "      <td>424428</td>\n",
       "      <td>0.7831</td>\n",
       "      <td>1.2976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6019</td>\n",
       "      <td>3.2370</td>\n",
       "      <td>2.9574</td>\n",
       "      <td>4.2287</td>\n",
       "      <td>-5.919126e+10</td>\n",
       "      <td>-50714214400</td>\n",
       "      <td>-39922372608</td>\n",
       "      <td>58.7255</td>\n",
       "      <td>54.9554</td>\n",
       "      <td>47.8400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>338136</td>\n",
       "      <td>2085.144</td>\n",
       "      <td>723.8198</td>\n",
       "      <td>595.2073</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>656.1464</td>\n",
       "      <td>0.9974</td>\n",
       "      <td>339014</td>\n",
       "      <td>0.7795</td>\n",
       "      <td>1.2161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4134</td>\n",
       "      <td>2.6228</td>\n",
       "      <td>2.6350</td>\n",
       "      <td>3.1704</td>\n",
       "      <td>-3.423307e+10</td>\n",
       "      <td>-37462601728</td>\n",
       "      <td>-31477794816</td>\n",
       "      <td>50.0259</td>\n",
       "      <td>52.8168</td>\n",
       "      <td>47.8315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>526843</td>\n",
       "      <td>2647.394</td>\n",
       "      <td>940.7379</td>\n",
       "      <td>715.3638</td>\n",
       "      <td>0.6494</td>\n",
       "      <td>819.0222</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>528876</td>\n",
       "      <td>0.7657</td>\n",
       "      <td>1.3150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9183</td>\n",
       "      <td>3.7516</td>\n",
       "      <td>3.8611</td>\n",
       "      <td>4.7192</td>\n",
       "      <td>-9.394835e+10</td>\n",
       "      <td>-74738221056</td>\n",
       "      <td>-60311207936</td>\n",
       "      <td>65.4772</td>\n",
       "      <td>59.2860</td>\n",
       "      <td>51.9378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>416063</td>\n",
       "      <td>2351.210</td>\n",
       "      <td>827.9804</td>\n",
       "      <td>645.2988</td>\n",
       "      <td>0.6266</td>\n",
       "      <td>727.8378</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>418255</td>\n",
       "      <td>0.7759</td>\n",
       "      <td>1.2831</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8028</td>\n",
       "      <td>5.0401</td>\n",
       "      <td>8.6136</td>\n",
       "      <td>8.2618</td>\n",
       "      <td>-3.207431e+10</td>\n",
       "      <td>-32060925952</td>\n",
       "      <td>-29575010304</td>\n",
       "      <td>43.3900</td>\n",
       "      <td>44.1259</td>\n",
       "      <td>41.1882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>347562</td>\n",
       "      <td>2160.354</td>\n",
       "      <td>763.9877</td>\n",
       "      <td>582.8359</td>\n",
       "      <td>0.6465</td>\n",
       "      <td>665.2291</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>350797</td>\n",
       "      <td>0.7569</td>\n",
       "      <td>1.3108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8865</td>\n",
       "      <td>2.7016</td>\n",
       "      <td>2.9761</td>\n",
       "      <td>4.4146</td>\n",
       "      <td>-3.998097e+10</td>\n",
       "      <td>-35980042240</td>\n",
       "      <td>-25593278464</td>\n",
       "      <td>52.7743</td>\n",
       "      <td>50.9080</td>\n",
       "      <td>42.6666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AREA  PERIMETER  MAJOR_AXIS  MINOR_AXIS  ECCENTRICITY   EQDIASQ  \\\n",
       "0  422163   2378.908    837.8484    645.6693        0.6373  733.1539   \n",
       "1  338136   2085.144    723.8198    595.2073        0.5690  656.1464   \n",
       "2  526843   2647.394    940.7379    715.3638        0.6494  819.0222   \n",
       "3  416063   2351.210    827.9804    645.2988        0.6266  727.8378   \n",
       "4  347562   2160.354    763.9877    582.8359        0.6465  665.2291   \n",
       "\n",
       "   SOLIDITY  CONVEX_AREA  EXTENT  ASPECT_RATIO  ...  SkewRB  KurtosisRR  \\\n",
       "0    0.9947       424428  0.7831        1.2976  ...  0.6019      3.2370   \n",
       "1    0.9974       339014  0.7795        1.2161  ...  0.4134      2.6228   \n",
       "2    0.9962       528876  0.7657        1.3150  ...  0.9183      3.7516   \n",
       "3    0.9948       418255  0.7759        1.2831  ...  1.8028      5.0401   \n",
       "4    0.9908       350797  0.7569        1.3108  ...  0.8865      2.7016   \n",
       "\n",
       "   KurtosisRG  KurtosisRB     EntropyRR    EntropyRG    EntropyRB  ALLdaub4RR  \\\n",
       "0      2.9574      4.2287 -5.919126e+10 -50714214400 -39922372608     58.7255   \n",
       "1      2.6350      3.1704 -3.423307e+10 -37462601728 -31477794816     50.0259   \n",
       "2      3.8611      4.7192 -9.394835e+10 -74738221056 -60311207936     65.4772   \n",
       "3      8.6136      8.2618 -3.207431e+10 -32060925952 -29575010304     43.3900   \n",
       "4      2.9761      4.4146 -3.998097e+10 -35980042240 -25593278464     52.7743   \n",
       "\n",
       "   ALLdaub4RG  ALLdaub4RB  \n",
       "0     54.9554     47.8400  \n",
       "1     52.8168     47.8315  \n",
       "2     59.2860     51.9378  \n",
       "3     44.1259     41.1882  \n",
       "4     50.9080     42.6666  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = dataset.drop(columns='Class')\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "6159a6bb-9749-4a8b-89dc-eeac7ecf6d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [5],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6],\n",
       "       [6]])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the label\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and Transform the labels into integers\n",
    "encoded_label = label_encoder.fit_transform(label)\n",
    "\n",
    "# Convert integer labels to one-hot encoding\n",
    "#encoded_label = to_categorical(encoded_label)\n",
    "\n",
    "# Reshape the labels into a 2D array\n",
    "encoded_label = encoded_label.reshape(-1, 1)\n",
    "\n",
    "encoded_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "1f63e371-d8f5-4ee3-885a-8c17926e9fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Encoded Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>898 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Encoded Label\n",
       "0                0\n",
       "1                0\n",
       "2                0\n",
       "3                0\n",
       "4                0\n",
       "..             ...\n",
       "893              6\n",
       "894              6\n",
       "895              6\n",
       "896              6\n",
       "897              6\n",
       "\n",
       "[898 rows x 1 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame for the encoded labels \n",
    "encoded_labels_dataset = pd.DataFrame(encoded_label, columns=['Encoded Label'])\n",
    "encoded_labels_dataset\n",
    "#dataset['Encoded Label'] = encoded_labels_dataset\n",
    "#dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "3ec0ca5a-fe52-401b-9315-2f0bc37f7fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoding mapping: {'BERHI': 0, 'DEGLET': 1, 'DOKOL': 2, 'IRAQI': 3, 'ROTANA': 4, 'SAFAVI': 5, 'SOGAY': 6}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Encoded Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERHI</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DEGLET</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DOKOL</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IRAQI</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ROTANA</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SAFAVI</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SOGAY</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Class  Encoded Label\n",
       "0   BERHI              0\n",
       "1  DEGLET              1\n",
       "2   DOKOL              2\n",
       "3   IRAQI              3\n",
       "4  ROTANA              4\n",
       "5  SAFAVI              5\n",
       "6   SOGAY              6"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the mapping of labels to integers\n",
    "label_mapping = dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))\n",
    "print(\"Label encoding mapping:\", label_mapping)\n",
    "data_label = pd.DataFrame(list(label_mapping.items()), columns=['Class','Encoded Label'])\n",
    "data_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "7fb8fe07-5883-4401-af8c-1f0dd0d537ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AREA</th>\n",
       "      <th>PERIMETER</th>\n",
       "      <th>MAJOR_AXIS</th>\n",
       "      <th>MINOR_AXIS</th>\n",
       "      <th>ECCENTRICITY</th>\n",
       "      <th>EQDIASQ</th>\n",
       "      <th>SOLIDITY</th>\n",
       "      <th>CONVEX_AREA</th>\n",
       "      <th>EXTENT</th>\n",
       "      <th>ASPECT_RATIO</th>\n",
       "      <th>...</th>\n",
       "      <th>SkewRB</th>\n",
       "      <th>KurtosisRR</th>\n",
       "      <th>KurtosisRG</th>\n",
       "      <th>KurtosisRB</th>\n",
       "      <th>EntropyRR</th>\n",
       "      <th>EntropyRG</th>\n",
       "      <th>EntropyRB</th>\n",
       "      <th>ALLdaub4RR</th>\n",
       "      <th>ALLdaub4RG</th>\n",
       "      <th>ALLdaub4RB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.772274</td>\n",
       "      <td>0.772079</td>\n",
       "      <td>0.565604</td>\n",
       "      <td>0.841941</td>\n",
       "      <td>0.446429</td>\n",
       "      <td>0.871512</td>\n",
       "      <td>0.983209</td>\n",
       "      <td>0.767108</td>\n",
       "      <td>0.787438</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>...</td>\n",
       "      <td>0.395739</td>\n",
       "      <td>0.062495</td>\n",
       "      <td>0.053715</td>\n",
       "      <td>0.080752</td>\n",
       "      <td>0.458251</td>\n",
       "      <td>0.455197</td>\n",
       "      <td>0.546327</td>\n",
       "      <td>0.673513</td>\n",
       "      <td>0.550537</td>\n",
       "      <td>0.494665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.617835</td>\n",
       "      <td>0.617480</td>\n",
       "      <td>0.436904</td>\n",
       "      <td>0.775906</td>\n",
       "      <td>0.342186</td>\n",
       "      <td>0.773229</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.611906</td>\n",
       "      <td>0.776970</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350002</td>\n",
       "      <td>0.037387</td>\n",
       "      <td>0.040885</td>\n",
       "      <td>0.046033</td>\n",
       "      <td>0.687311</td>\n",
       "      <td>0.599151</td>\n",
       "      <td>0.643352</td>\n",
       "      <td>0.538923</td>\n",
       "      <td>0.516341</td>\n",
       "      <td>0.494501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.964674</td>\n",
       "      <td>0.913374</td>\n",
       "      <td>0.681733</td>\n",
       "      <td>0.933143</td>\n",
       "      <td>0.464896</td>\n",
       "      <td>0.981104</td>\n",
       "      <td>0.992537</td>\n",
       "      <td>0.956896</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.472509</td>\n",
       "      <td>0.083531</td>\n",
       "      <td>0.089677</td>\n",
       "      <td>0.096843</td>\n",
       "      <td>0.139260</td>\n",
       "      <td>0.194220</td>\n",
       "      <td>0.312066</td>\n",
       "      <td>0.777967</td>\n",
       "      <td>0.619782</td>\n",
       "      <td>0.573507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.761063</td>\n",
       "      <td>0.757502</td>\n",
       "      <td>0.554467</td>\n",
       "      <td>0.841456</td>\n",
       "      <td>0.430098</td>\n",
       "      <td>0.864727</td>\n",
       "      <td>0.983831</td>\n",
       "      <td>0.755891</td>\n",
       "      <td>0.766502</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.687121</td>\n",
       "      <td>0.136202</td>\n",
       "      <td>0.278800</td>\n",
       "      <td>0.213061</td>\n",
       "      <td>0.707124</td>\n",
       "      <td>0.657830</td>\n",
       "      <td>0.665214</td>\n",
       "      <td>0.436260</td>\n",
       "      <td>0.377376</td>\n",
       "      <td>0.366683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.635159</td>\n",
       "      <td>0.657060</td>\n",
       "      <td>0.482240</td>\n",
       "      <td>0.759716</td>\n",
       "      <td>0.460470</td>\n",
       "      <td>0.784821</td>\n",
       "      <td>0.958955</td>\n",
       "      <td>0.633316</td>\n",
       "      <td>0.711253</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.464794</td>\n",
       "      <td>0.040608</td>\n",
       "      <td>0.054459</td>\n",
       "      <td>0.086850</td>\n",
       "      <td>0.634558</td>\n",
       "      <td>0.615256</td>\n",
       "      <td>0.710963</td>\n",
       "      <td>0.581443</td>\n",
       "      <td>0.485820</td>\n",
       "      <td>0.395128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       AREA  PERIMETER  MAJOR_AXIS  MINOR_AXIS  ECCENTRICITY   EQDIASQ  \\\n",
       "0  0.772274   0.772079    0.565604    0.841941      0.446429  0.871512   \n",
       "1  0.617835   0.617480    0.436904    0.775906      0.342186  0.773229   \n",
       "2  0.964674   0.913374    0.681733    0.933143      0.464896  0.981104   \n",
       "3  0.761063   0.757502    0.554467    0.841456      0.430098  0.864727   \n",
       "4  0.635159   0.657060    0.482240    0.759716      0.460470  0.784821   \n",
       "\n",
       "   SOLIDITY  CONVEX_AREA    EXTENT  ASPECT_RATIO  ...    SkewRB  KurtosisRR  \\\n",
       "0  0.983209     0.767108  0.787438      0.000435  ...  0.395739    0.062495   \n",
       "1  1.000000     0.611906  0.776970      0.000282  ...  0.350002    0.037387   \n",
       "2  0.992537     0.956896  0.736842      0.000467  ...  0.472509    0.083531   \n",
       "3  0.983831     0.755891  0.766502      0.000408  ...  0.687121    0.136202   \n",
       "4  0.958955     0.633316  0.711253      0.000459  ...  0.464794    0.040608   \n",
       "\n",
       "   KurtosisRG  KurtosisRB  EntropyRR  EntropyRG  EntropyRB  ALLdaub4RR  \\\n",
       "0    0.053715    0.080752   0.458251   0.455197   0.546327    0.673513   \n",
       "1    0.040885    0.046033   0.687311   0.599151   0.643352    0.538923   \n",
       "2    0.089677    0.096843   0.139260   0.194220   0.312066    0.777967   \n",
       "3    0.278800    0.213061   0.707124   0.657830   0.665214    0.436260   \n",
       "4    0.054459    0.086850   0.634558   0.615256   0.710963    0.581443   \n",
       "\n",
       "   ALLdaub4RG  ALLdaub4RB  \n",
       "0    0.550537    0.494665  \n",
       "1    0.516341    0.494501  \n",
       "2    0.619782    0.573507  \n",
       "3    0.377376    0.366683  \n",
       "4    0.485820    0.395128  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale the Features\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the features\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Convert scaled features back to a DataFrame if needed\n",
    "scaled_features_dataset = pd.DataFrame(scaled_features, columns=features.columns)\n",
    "\n",
    "scaled_features_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb00047-3b50-4ffc-ae77-460b47324858",
   "metadata": {},
   "source": [
    "## Spliting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "889a9d9b-5742-402c-8197-eea05fe5cde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (628, 34), (628, 1)\n",
      "Testing set: (135, 34), (135, 1)\n",
      "Validation set: (135, 34), (135, 1)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training (70%), testing (15%), and validation (15%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(scaled_features, encoded_label, test_size=0.30, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.50, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing set: {X_test.shape}, {y_test.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}, {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cec8b59-17b2-4085-a93a-9cd4f11c5132",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "Label Encoding Mapping: This step maps each category to a corresponding integer.\n",
    "##### Example mapping: {'Class_A': 0, 'Class_B': 1, 'Class_C': 2}\n",
    "    \n",
    "##### Feature Scaling: Features are scaled using MinMaxScaler to normalize the range of each feature between 0 and 1.\n",
    "    \n",
    "##### Data Splitting: The dataset is divided into three parts:\n",
    "\n",
    "##### Training set: 70% of the data used for training the model.\n",
    "##### Testing set: 15% of the data used for evaluating the model's performance.\n",
    "##### Validation set: 15% of the data used for tuning the model.\n",
    "                                    \n",
    "This process ensures the data is well-prepared for training learning models, with proper label encoding, feature scaling, and balanced data splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d376a8-0769-49ef-b567-d2e65daa0c28",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "bc0ab985-acf9-4a34-9d3a-09953d46832a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_105 (Dense)           (None, 64)                2240      \n",
      "                                                                 \n",
      " dense_106 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_107 (Dense)           (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4650 (18.16 KB)\n",
      "Trainable params: 4650 (18.16 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Get the number of features from scaled_features\n",
    "num_features = scaled_features.shape[1] \n",
    "\n",
    "# Add the input layer and the first hidden layer\n",
    "# Replace num_features with the number of features in the dataset\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(num_features,))) \n",
    "\n",
    "# Add a second hidden layer\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "\n",
    "# Add the output layer and use 5 units with softmax activation\n",
    "model.add(layers.Dense(7, activation='softmax')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "e5656fe6-e849-49bb-a07d-236848e5bc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_105 (Dense)           (None, 64)                2240      \n",
      "                                                                 \n",
      " dense_106 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_107 (Dense)           (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4650 (18.16 KB)\n",
      "Trainable params: 4650 (18.16 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a946ed00-9172-45d8-b62e-54b9ff07a2c5",
   "metadata": {},
   "source": [
    "### Train and Predict on the Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "76003360-b237-45a4-a6b3-9b9753cfea0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(628, 34)\n",
      "(628, 1)\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4324 - accuracy: 0.8599\n",
      "Model Accuacy on Train set:  0.8598726391792297\n",
      "Model Loss on Train set:  0.43236809968948364\n"
     ]
    }
   ],
   "source": [
    "#y_train_encoded = to_categorical(y_train, num_classes=num_classes)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "training = model.fit(X_train, y_train, epochs=10, verbose=0)\n",
    "loss ,accuracy =model.evaluate(X_train, y_train, verbose=1)\n",
    "\n",
    "print(\"Model Accuacy on Train set: \", accuracy) \n",
    "print(\"Model Loss on Train set: \", loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "7515e2ad-4620-4253-9204-dcc6dda3401a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7174 - accuracy: 0.7778\n",
      "5/5 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model on validation set\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=1)\n",
    "\n",
    "# Predictions and confusion matrix\n",
    "y_val_pred = np.argmax(model.predict(X_val), axis=-1)\n",
    "y_val_true = np.argmax(y_val, axis=-1)\n",
    "conf_matrix = confusion_matrix(y_val_true, y_val_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "0edb6591-a62c-45f2-af3a-6e8fe00c7757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2, 14, 35,  9, 36, 27, 12],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0]], dtype=int64)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the confusion Matrix\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a381d70b-38e8-479d-9b29-28b79b67c677",
   "metadata": {},
   "source": [
    "## Create and Train 5 other Model, Record the Accuracy and Select Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "3d5027fd-8bb3-4d0d-874f-a1983d002421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to build, compile, train, and evaluate a model\n",
    "def create_and_evaluate_model(num_of_layers, activation, optimizer, loss, epochs=10, batch_size=16):\n",
    "    new_model = tf.keras.Sequential()\n",
    "\n",
    "    # Get the number of features from scaled_features\n",
    "    num_features = scaled_features.shape[1] \n",
    "     # Ensure labels are one-hot encoded\n",
    "    #y_train_encoded = to_categorical(y_train, num_classes=num_classes)\n",
    "    #y_val_encoded = to_categorical(y_val, num_classes=num_classes)\n",
    "    #print(y_train_encoded.shape)\n",
    "    #print(X_train.shape)\n",
    "    #print(y_train.shape)\n",
    "    \n",
    "    # Add the input layer and the first hidden layer\n",
    "    # Replace num_features with the number of features in the dataset\n",
    "    new_model.add(layers.Dense(64, activation='relu', input_shape=(num_features,))) \n",
    "    \n",
    "    for units in num_of_layers:\n",
    "        # Add another hidden layer\n",
    "        new_model.add(layers.Dense(units, activation=activation))\n",
    "\n",
    "    \n",
    "    # Add the output layer with softmax activation for a multiclass data\n",
    "    new_model.add(layers.Dense(7, activation='softmax')) \n",
    "    \n",
    "    new_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    \n",
    "    new_model.summary()\n",
    "    # Train the model\n",
    "    training = new_model.fit(X_train, y_train, epochs=epochs, verbose=1)\n",
    "    \n",
    "    # Evaluate the model on validation set\n",
    "    val_loss, val_accuracy = new_model.evaluate(X_val, y_val, verbose=1)\n",
    "    \n",
    "    # Predictions and confusion matrix\n",
    "    y_val_pred = np.argmax(new_model.predict(X_val), axis=-1)\n",
    "    y_val_true = np.argmax(y_val, axis=-1)\n",
    "    conf_matrix = confusion_matrix(y_val_true, y_val_pred)\n",
    "    \n",
    "    return {\n",
    "        \"model\": new_model,\n",
    "        \"val_accuracy\": val_accuracy,\n",
    "        \"conf_matrix\": conf_matrix, \n",
    "        \"loss\": val_loss\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "7be77f17-0472-4983-b8df-8f04dc784dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define five different sets of hyperparameters for the models\n",
    "hyperparams_list = [\n",
    "    {\"num_of_layers\": [64, 32], \"activation\": \"relu\", \"optimizer\": \"adam\", \"loss\": \"sparse_categorical_crossentropy\", \"epochs\":50},\n",
    "    {\"num_of_layers\": [128, 64, 32], \"activation\": \"tanh\", \"optimizer\": \"sgd\", \"loss\": \"sparse_categorical_crossentropy\", \"epochs\":70},\n",
    "    {\"num_of_layers\": [256, 128, 64], \"activation\": \"relu\", \"optimizer\": \"adam\", \"loss\": \"sparse_categorical_crossentropy\", \"epochs\":80},\n",
    "    {\"num_of_layers\": [128, 64], \"activation\": \"relu\", \"optimizer\": \"rmsprop\", \"loss\": \"sparse_categorical_crossentropy\", \"epochs\":90},\n",
    "    {\"num_of_layers\": [512, 256, 128], \"activation\": \"relu\", \"optimizer\": tf.keras.optimizers.Adam(learning_rate=0.0001), \"loss\": \"sparse_categorical_crossentropy\", \"epochs\":110}\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "40684f01-ae9c-4336-83fb-694ef87f3801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_170 (Dense)           (None, 64)                2240      \n",
      "                                                                 \n",
      " dense_171 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_172 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_173 (Dense)           (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8810 (34.41 KB)\n",
      "Trainable params: 8810 (34.41 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "20/20 [==============================] - 2s 4ms/step - loss: 2.1912 - accuracy: 0.1656\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.9427 - accuracy: 0.3264\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.5950 - accuracy: 0.6019\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.2612 - accuracy: 0.6194\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.0425 - accuracy: 0.6417\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.8968 - accuracy: 0.6561\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.7763 - accuracy: 0.7086\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6866 - accuracy: 0.7564\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.6219 - accuracy: 0.7564\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.5629 - accuracy: 0.7930\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.5321 - accuracy: 0.8137\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4905 - accuracy: 0.8041\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4720 - accuracy: 0.8073\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4416 - accuracy: 0.8408\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4380 - accuracy: 0.8344\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4181 - accuracy: 0.8455\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4140 - accuracy: 0.8392\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3830 - accuracy: 0.8535\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3976 - accuracy: 0.8503\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3791 - accuracy: 0.8535\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3637 - accuracy: 0.8631\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3655 - accuracy: 0.8551\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3477 - accuracy: 0.8694\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3624 - accuracy: 0.8567\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3415 - accuracy: 0.8646\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3348 - accuracy: 0.8694\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3325 - accuracy: 0.8790\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3281 - accuracy: 0.8615\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3223 - accuracy: 0.8822\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3168 - accuracy: 0.8806\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.3118 - accuracy: 0.8758\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3024 - accuracy: 0.8869\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3021 - accuracy: 0.8901\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2956 - accuracy: 0.8806\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3024 - accuracy: 0.8838\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3170 - accuracy: 0.8742\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2893 - accuracy: 0.8997\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2822 - accuracy: 0.8885\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2947 - accuracy: 0.8838\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2748 - accuracy: 0.8965\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2821 - accuracy: 0.8997\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2915 - accuracy: 0.8869\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.2894 - accuracy: 0.8822\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2671 - accuracy: 0.8997\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2587 - accuracy: 0.9076\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2687 - accuracy: 0.8965\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2631 - accuracy: 0.9013\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2644 - accuracy: 0.8981\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2673 - accuracy: 0.9013\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 0s 10ms/step - loss: 0.2584 - accuracy: 0.9045\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3014 - accuracy: 0.9111\n",
      "5/5 [==============================] - 0s 808us/step\n",
      "Model 1: Validation Accuracy = 0.9111\n",
      "Confusion Matrix:\n",
      "[[ 8 20 29 12 28 21 17]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]]\n",
      "\n",
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_174 (Dense)           (None, 64)                2240      \n",
      "                                                                 \n",
      " dense_175 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_176 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_177 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_178 (Dense)           (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21226 (82.91 KB)\n",
      "Trainable params: 21226 (82.91 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/70\n",
      "20/20 [==============================] - 1s 4ms/step - loss: 2.1439 - accuracy: 0.2261\n",
      "Epoch 2/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.8923 - accuracy: 0.4968\n",
      "Epoch 3/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.7787 - accuracy: 0.5892\n",
      "Epoch 4/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.6831 - accuracy: 0.6226\n",
      "Epoch 5/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.5904 - accuracy: 0.6115\n",
      "Epoch 6/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.5054 - accuracy: 0.6258\n",
      "Epoch 7/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.4229 - accuracy: 0.6226\n",
      "Epoch 8/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.3474 - accuracy: 0.6258\n",
      "Epoch 9/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.2805 - accuracy: 0.6274\n",
      "Epoch 10/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.2196 - accuracy: 0.6290\n",
      "Epoch 11/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.1677 - accuracy: 0.6290\n",
      "Epoch 12/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 1.1181 - accuracy: 0.6290\n",
      "Epoch 13/70\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.0760 - accuracy: 0.6290\n",
      "Epoch 14/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.0365 - accuracy: 0.6290\n",
      "Epoch 15/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.0022 - accuracy: 0.6401\n",
      "Epoch 16/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.9689 - accuracy: 0.6369\n",
      "Epoch 17/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.9411 - accuracy: 0.6481\n",
      "Epoch 18/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.9132 - accuracy: 0.6592\n",
      "Epoch 19/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.8921 - accuracy: 0.6688\n",
      "Epoch 20/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.8681 - accuracy: 0.6736\n",
      "Epoch 21/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.8464 - accuracy: 0.6911\n",
      "Epoch 22/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.8266 - accuracy: 0.6943\n",
      "Epoch 23/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.8088 - accuracy: 0.6990\n",
      "Epoch 24/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.7943 - accuracy: 0.7102\n",
      "Epoch 25/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.7779 - accuracy: 0.7229\n",
      "Epoch 26/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.7632 - accuracy: 0.7357\n",
      "Epoch 27/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.7485 - accuracy: 0.7309\n",
      "Epoch 28/70\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.7362 - accuracy: 0.7548\n",
      "Epoch 29/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.7223 - accuracy: 0.7548\n",
      "Epoch 30/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.7130 - accuracy: 0.7564\n",
      "Epoch 31/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.7003 - accuracy: 0.7500\n",
      "Epoch 32/70\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6923 - accuracy: 0.7675\n",
      "Epoch 33/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6795 - accuracy: 0.7707\n",
      "Epoch 34/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6726 - accuracy: 0.7707\n",
      "Epoch 35/70\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.6608 - accuracy: 0.7771\n",
      "Epoch 36/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6512 - accuracy: 0.7787\n",
      "Epoch 37/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6429 - accuracy: 0.7803\n",
      "Epoch 38/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6355 - accuracy: 0.7818\n",
      "Epoch 39/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6279 - accuracy: 0.7787\n",
      "Epoch 40/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6179 - accuracy: 0.7962\n",
      "Epoch 41/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6131 - accuracy: 0.7962\n",
      "Epoch 42/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6042 - accuracy: 0.7866\n",
      "Epoch 43/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5981 - accuracy: 0.7978\n",
      "Epoch 44/70\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.5897 - accuracy: 0.8041\n",
      "Epoch 45/70\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.5834 - accuracy: 0.8025\n",
      "Epoch 46/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5774 - accuracy: 0.8121\n",
      "Epoch 47/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5682 - accuracy: 0.8089\n",
      "Epoch 48/70\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.5617 - accuracy: 0.8232\n",
      "Epoch 49/70\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.5573 - accuracy: 0.8121\n",
      "Epoch 50/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5512 - accuracy: 0.8169\n",
      "Epoch 51/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5442 - accuracy: 0.8201\n",
      "Epoch 52/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5408 - accuracy: 0.8121\n",
      "Epoch 53/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5353 - accuracy: 0.8280\n",
      "Epoch 54/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5291 - accuracy: 0.8248\n",
      "Epoch 55/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5243 - accuracy: 0.8296\n",
      "Epoch 56/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.5178 - accuracy: 0.8232\n",
      "Epoch 57/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5147 - accuracy: 0.8185\n",
      "Epoch 58/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5061 - accuracy: 0.8360\n",
      "Epoch 59/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5048 - accuracy: 0.8312\n",
      "Epoch 60/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4984 - accuracy: 0.8280\n",
      "Epoch 61/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4934 - accuracy: 0.8408\n",
      "Epoch 62/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4908 - accuracy: 0.8408\n",
      "Epoch 63/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4852 - accuracy: 0.8408\n",
      "Epoch 64/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4805 - accuracy: 0.8455\n",
      "Epoch 65/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4730 - accuracy: 0.8487\n",
      "Epoch 66/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4710 - accuracy: 0.8439\n",
      "Epoch 67/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4634 - accuracy: 0.8503\n",
      "Epoch 68/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4613 - accuracy: 0.8471\n",
      "Epoch 69/70\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4568 - accuracy: 0.8535\n",
      "Epoch 70/70\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4536 - accuracy: 0.8551\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4911 - accuracy: 0.8741\n",
      "5/5 [==============================] - 0s 6ms/step\n",
      "Model 2: Validation Accuracy = 0.8741\n",
      "Confusion Matrix:\n",
      "[[ 7 23 29 13 30 21 12]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]]\n",
      "\n",
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_179 (Dense)           (None, 64)                2240      \n",
      "                                                                 \n",
      " dense_180 (Dense)           (None, 256)               16640     \n",
      "                                                                 \n",
      " dense_181 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_182 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_183 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60682 (237.04 KB)\n",
      "Trainable params: 60682 (237.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/80\n",
      "20/20 [==============================] - 2s 3ms/step - loss: 1.9073 - accuracy: 0.4299\n",
      "Epoch 2/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.2806 - accuracy: 0.6083\n",
      "Epoch 3/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.8612 - accuracy: 0.6752\n",
      "Epoch 4/80\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.6611 - accuracy: 0.7452\n",
      "Epoch 5/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5592 - accuracy: 0.7882\n",
      "Epoch 6/80\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.5054 - accuracy: 0.8057\n",
      "Epoch 7/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4990 - accuracy: 0.7898\n",
      "Epoch 8/80\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4488 - accuracy: 0.8025\n",
      "Epoch 9/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4003 - accuracy: 0.8455\n",
      "Epoch 10/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4004 - accuracy: 0.8344\n",
      "Epoch 11/80\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.3746 - accuracy: 0.8392\n",
      "Epoch 12/80\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.3875 - accuracy: 0.8471\n",
      "Epoch 13/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4178 - accuracy: 0.8185\n",
      "Epoch 14/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3494 - accuracy: 0.8551\n",
      "Epoch 15/80\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3278 - accuracy: 0.8694\n",
      "Epoch 16/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3401 - accuracy: 0.8646\n",
      "Epoch 17/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3817 - accuracy: 0.8424\n",
      "Epoch 18/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3506 - accuracy: 0.8583\n",
      "Epoch 19/80\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.3414 - accuracy: 0.8599\n",
      "Epoch 20/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3063 - accuracy: 0.8806\n",
      "Epoch 21/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3123 - accuracy: 0.8790\n",
      "Epoch 22/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2940 - accuracy: 0.8838\n",
      "Epoch 23/80\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.3007 - accuracy: 0.8854\n",
      "Epoch 24/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3162 - accuracy: 0.8742\n",
      "Epoch 25/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2871 - accuracy: 0.8965\n",
      "Epoch 26/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2692 - accuracy: 0.8949\n",
      "Epoch 27/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3115 - accuracy: 0.8854\n",
      "Epoch 28/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2650 - accuracy: 0.8917\n",
      "Epoch 29/80\n",
      "20/20 [==============================] - 0s 10ms/step - loss: 0.2701 - accuracy: 0.8981\n",
      "Epoch 30/80\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.2837 - accuracy: 0.8901\n",
      "Epoch 31/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2919 - accuracy: 0.8885\n",
      "Epoch 32/80\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.2662 - accuracy: 0.9013\n",
      "Epoch 33/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2752 - accuracy: 0.9013\n",
      "Epoch 34/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2625 - accuracy: 0.8917\n",
      "Epoch 35/80\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.2599 - accuracy: 0.8997\n",
      "Epoch 36/80\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2510 - accuracy: 0.9076\n",
      "Epoch 37/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2462 - accuracy: 0.8965\n",
      "Epoch 38/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2348 - accuracy: 0.9156\n",
      "Epoch 39/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2563 - accuracy: 0.8933\n",
      "Epoch 40/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2322 - accuracy: 0.9124\n",
      "Epoch 41/80\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.2525 - accuracy: 0.8933\n",
      "Epoch 42/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2361 - accuracy: 0.9076\n",
      "Epoch 43/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2317 - accuracy: 0.9108\n",
      "Epoch 44/80\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.2161 - accuracy: 0.9045\n",
      "Epoch 45/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2359 - accuracy: 0.9045\n",
      "Epoch 46/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2201 - accuracy: 0.9092\n",
      "Epoch 47/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2355 - accuracy: 0.9108\n",
      "Epoch 48/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2413 - accuracy: 0.9061\n",
      "Epoch 49/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2304 - accuracy: 0.9156\n",
      "Epoch 50/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2538 - accuracy: 0.9029\n",
      "Epoch 51/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2085 - accuracy: 0.9220\n",
      "Epoch 52/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2027 - accuracy: 0.9220\n",
      "Epoch 53/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2046 - accuracy: 0.9283\n",
      "Epoch 54/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2260 - accuracy: 0.9092\n",
      "Epoch 55/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1978 - accuracy: 0.9188\n",
      "Epoch 56/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1893 - accuracy: 0.9347\n",
      "Epoch 57/80\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.1827 - accuracy: 0.9283\n",
      "Epoch 58/80\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.2108 - accuracy: 0.9156\n",
      "Epoch 59/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2421 - accuracy: 0.8949\n",
      "Epoch 60/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2141 - accuracy: 0.9076\n",
      "Epoch 61/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1957 - accuracy: 0.9220\n",
      "Epoch 62/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1712 - accuracy: 0.9299\n",
      "Epoch 63/80\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.1839 - accuracy: 0.9299\n",
      "Epoch 64/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1776 - accuracy: 0.9347\n",
      "Epoch 65/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1769 - accuracy: 0.9331\n",
      "Epoch 66/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1829 - accuracy: 0.9347\n",
      "Epoch 67/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1679 - accuracy: 0.9331\n",
      "Epoch 68/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1602 - accuracy: 0.9379\n",
      "Epoch 69/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1654 - accuracy: 0.9411\n",
      "Epoch 70/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2084 - accuracy: 0.8997\n",
      "Epoch 71/80\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.1618 - accuracy: 0.9363\n",
      "Epoch 72/80\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.1555 - accuracy: 0.9443\n",
      "Epoch 73/80\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.1700 - accuracy: 0.9252\n",
      "Epoch 74/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1561 - accuracy: 0.9411\n",
      "Epoch 75/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1612 - accuracy: 0.9411\n",
      "Epoch 76/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1581 - accuracy: 0.9443\n",
      "Epoch 77/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1547 - accuracy: 0.9395\n",
      "Epoch 78/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1532 - accuracy: 0.9427\n",
      "Epoch 79/80\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1765 - accuracy: 0.9315\n",
      "Epoch 80/80\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1691 - accuracy: 0.9395\n",
      "5/5 [==============================] - 0s 881us/step - loss: 0.2456 - accuracy: 0.9037\n",
      "5/5 [==============================] - 0s 0s/step\n",
      "Model 3: Validation Accuracy = 0.9037\n",
      "Confusion Matrix:\n",
      "[[ 7 20 28 15 28 21 16]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]]\n",
      "\n",
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_184 (Dense)           (None, 64)                2240      \n",
      "                                                                 \n",
      " dense_185 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_186 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_187 (Dense)           (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19466 (76.04 KB)\n",
      "Trainable params: 19466 (76.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/90\n",
      "20/20 [==============================] - 1s 4ms/step - loss: 1.8292 - accuracy: 0.4761\n",
      "Epoch 2/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.2216 - accuracy: 0.6226\n",
      "Epoch 3/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.9603 - accuracy: 0.6401\n",
      "Epoch 4/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.8040 - accuracy: 0.7070\n",
      "Epoch 5/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.7010 - accuracy: 0.7500\n",
      "Epoch 6/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6367 - accuracy: 0.7739\n",
      "Epoch 7/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5812 - accuracy: 0.7962\n",
      "Epoch 8/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5436 - accuracy: 0.7914\n",
      "Epoch 9/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5207 - accuracy: 0.7994\n",
      "Epoch 10/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4871 - accuracy: 0.8264\n",
      "Epoch 11/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4628 - accuracy: 0.8264\n",
      "Epoch 12/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4465 - accuracy: 0.8280\n",
      "Epoch 13/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4526 - accuracy: 0.8264\n",
      "Epoch 14/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4224 - accuracy: 0.8344\n",
      "Epoch 15/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4163 - accuracy: 0.8455\n",
      "Epoch 16/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4024 - accuracy: 0.8535\n",
      "Epoch 17/90\n",
      "20/20 [==============================] - 0s 10ms/step - loss: 0.4088 - accuracy: 0.8424\n",
      "Epoch 18/90\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.3947 - accuracy: 0.8360\n",
      "Epoch 19/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3829 - accuracy: 0.8551\n",
      "Epoch 20/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3934 - accuracy: 0.8439\n",
      "Epoch 21/90\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3650 - accuracy: 0.8567\n",
      "Epoch 22/90\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3545 - accuracy: 0.8631\n",
      "Epoch 23/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3648 - accuracy: 0.8631\n",
      "Epoch 24/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3606 - accuracy: 0.8567\n",
      "Epoch 25/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3554 - accuracy: 0.8567\n",
      "Epoch 26/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3452 - accuracy: 0.8631\n",
      "Epoch 27/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3301 - accuracy: 0.8662\n",
      "Epoch 28/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3304 - accuracy: 0.8742\n",
      "Epoch 29/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3540 - accuracy: 0.8678\n",
      "Epoch 30/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3280 - accuracy: 0.8758\n",
      "Epoch 31/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3229 - accuracy: 0.8806\n",
      "Epoch 32/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3120 - accuracy: 0.8854\n",
      "Epoch 33/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3168 - accuracy: 0.8806\n",
      "Epoch 34/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3163 - accuracy: 0.8646\n",
      "Epoch 35/90\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3040 - accuracy: 0.8822\n",
      "Epoch 36/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3052 - accuracy: 0.8885\n",
      "Epoch 37/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2848 - accuracy: 0.8917\n",
      "Epoch 38/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3031 - accuracy: 0.8854\n",
      "Epoch 39/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2871 - accuracy: 0.8838\n",
      "Epoch 40/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2902 - accuracy: 0.8774\n",
      "Epoch 41/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2785 - accuracy: 0.8981\n",
      "Epoch 42/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2779 - accuracy: 0.8917\n",
      "Epoch 43/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2869 - accuracy: 0.8838\n",
      "Epoch 44/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2714 - accuracy: 0.8965\n",
      "Epoch 45/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2681 - accuracy: 0.8933\n",
      "Epoch 46/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2698 - accuracy: 0.9013\n",
      "Epoch 47/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2548 - accuracy: 0.9045\n",
      "Epoch 48/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2717 - accuracy: 0.8885\n",
      "Epoch 49/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2617 - accuracy: 0.9029\n",
      "Epoch 50/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2749 - accuracy: 0.8917\n",
      "Epoch 51/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2572 - accuracy: 0.9061\n",
      "Epoch 52/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2521 - accuracy: 0.9045\n",
      "Epoch 53/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2518 - accuracy: 0.9029\n",
      "Epoch 54/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2492 - accuracy: 0.9029\n",
      "Epoch 55/90\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2515 - accuracy: 0.9124\n",
      "Epoch 56/90\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2303 - accuracy: 0.9188\n",
      "Epoch 57/90\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2413 - accuracy: 0.9045\n",
      "Epoch 58/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2334 - accuracy: 0.9140\n",
      "Epoch 59/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2551 - accuracy: 0.9124\n",
      "Epoch 60/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2349 - accuracy: 0.9076\n",
      "Epoch 61/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2454 - accuracy: 0.9108\n",
      "Epoch 62/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2316 - accuracy: 0.9076\n",
      "Epoch 63/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2214 - accuracy: 0.9140\n",
      "Epoch 64/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2389 - accuracy: 0.9156\n",
      "Epoch 65/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2220 - accuracy: 0.9156\n",
      "Epoch 66/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2247 - accuracy: 0.9140\n",
      "Epoch 67/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2247 - accuracy: 0.9172\n",
      "Epoch 68/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2476 - accuracy: 0.8965\n",
      "Epoch 69/90\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2232 - accuracy: 0.9124\n",
      "Epoch 70/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2218 - accuracy: 0.9172\n",
      "Epoch 71/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2150 - accuracy: 0.9252\n",
      "Epoch 72/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2219 - accuracy: 0.9076\n",
      "Epoch 73/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2172 - accuracy: 0.9252\n",
      "Epoch 74/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2149 - accuracy: 0.9140\n",
      "Epoch 75/90\n",
      "20/20 [==============================] - 0s 10ms/step - loss: 0.2044 - accuracy: 0.9236\n",
      "Epoch 76/90\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.2233 - accuracy: 0.9124\n",
      "Epoch 77/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2233 - accuracy: 0.9172\n",
      "Epoch 78/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1973 - accuracy: 0.9172\n",
      "Epoch 79/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2235 - accuracy: 0.9108\n",
      "Epoch 80/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1887 - accuracy: 0.9188\n",
      "Epoch 81/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2199 - accuracy: 0.9124\n",
      "Epoch 82/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2134 - accuracy: 0.9172\n",
      "Epoch 83/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1995 - accuracy: 0.9220\n",
      "Epoch 84/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2149 - accuracy: 0.9268\n",
      "Epoch 85/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2017 - accuracy: 0.9252\n",
      "Epoch 86/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2104 - accuracy: 0.9172\n",
      "Epoch 87/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1926 - accuracy: 0.9252\n",
      "Epoch 88/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1983 - accuracy: 0.9188\n",
      "Epoch 89/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1961 - accuracy: 0.9156\n",
      "Epoch 90/90\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1933 - accuracy: 0.9268\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2175 - accuracy: 0.9407\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "Model 4: Validation Accuracy = 0.9407\n",
      "Confusion Matrix:\n",
      "[[ 7 16 29 15 29 20 19]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]]\n",
      "\n",
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_188 (Dense)           (None, 64)                2240      \n",
      "                                                                 \n",
      " dense_189 (Dense)           (None, 512)               33280     \n",
      "                                                                 \n",
      " dense_190 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_191 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_192 (Dense)           (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 201034 (785.29 KB)\n",
      "Trainable params: 201034 (785.29 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/110\n",
      "20/20 [==============================] - 1s 3ms/step - loss: 2.1596 - accuracy: 0.3519\n",
      "Epoch 2/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 1.9444 - accuracy: 0.6003\n",
      "Epoch 3/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.7500 - accuracy: 0.6290\n",
      "Epoch 4/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.5586 - accuracy: 0.6099\n",
      "Epoch 5/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.3564 - accuracy: 0.6226\n",
      "Epoch 6/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.1814 - accuracy: 0.6258\n",
      "Epoch 7/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 1.0315 - accuracy: 0.6274\n",
      "Epoch 8/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.9158 - accuracy: 0.6433\n",
      "Epoch 9/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.8210 - accuracy: 0.6752\n",
      "Epoch 10/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.7449 - accuracy: 0.7516\n",
      "Epoch 11/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.6821 - accuracy: 0.7548\n",
      "Epoch 12/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.6312 - accuracy: 0.7803\n",
      "Epoch 13/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.5900 - accuracy: 0.7914\n",
      "Epoch 14/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.5582 - accuracy: 0.7850\n",
      "Epoch 15/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.5403 - accuracy: 0.7946\n",
      "Epoch 16/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.5163 - accuracy: 0.8232\n",
      "Epoch 17/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4858 - accuracy: 0.8217\n",
      "Epoch 18/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4675 - accuracy: 0.8312\n",
      "Epoch 19/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4556 - accuracy: 0.8408\n",
      "Epoch 20/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4404 - accuracy: 0.8408\n",
      "Epoch 21/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4337 - accuracy: 0.8360\n",
      "Epoch 22/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4134 - accuracy: 0.8503\n",
      "Epoch 23/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4052 - accuracy: 0.8439\n",
      "Epoch 24/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.3987 - accuracy: 0.8471\n",
      "Epoch 25/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.3908 - accuracy: 0.8535\n",
      "Epoch 26/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.3846 - accuracy: 0.8471\n",
      "Epoch 27/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.3723 - accuracy: 0.8567\n",
      "Epoch 28/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.3626 - accuracy: 0.8726\n",
      "Epoch 29/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.3601 - accuracy: 0.8615\n",
      "Epoch 30/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.3619 - accuracy: 0.8567\n",
      "Epoch 31/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.3603 - accuracy: 0.8535\n",
      "Epoch 32/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.3515 - accuracy: 0.8742\n",
      "Epoch 33/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.3476 - accuracy: 0.8742\n",
      "Epoch 34/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.3355 - accuracy: 0.8694\n",
      "Epoch 35/110\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.3349 - accuracy: 0.8758\n",
      "Epoch 36/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3316 - accuracy: 0.8885\n",
      "Epoch 37/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3370 - accuracy: 0.8774\n",
      "Epoch 38/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3277 - accuracy: 0.8790\n",
      "Epoch 39/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3258 - accuracy: 0.8742\n",
      "Epoch 40/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3151 - accuracy: 0.8965\n",
      "Epoch 41/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3144 - accuracy: 0.8854\n",
      "Epoch 42/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3138 - accuracy: 0.8822\n",
      "Epoch 43/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3138 - accuracy: 0.8901\n",
      "Epoch 44/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3027 - accuracy: 0.8885\n",
      "Epoch 45/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3044 - accuracy: 0.8917\n",
      "Epoch 46/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3050 - accuracy: 0.8854\n",
      "Epoch 47/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2977 - accuracy: 0.8981\n",
      "Epoch 48/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2992 - accuracy: 0.8981\n",
      "Epoch 49/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.3063 - accuracy: 0.8806\n",
      "Epoch 50/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2907 - accuracy: 0.9029\n",
      "Epoch 51/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2921 - accuracy: 0.8965\n",
      "Epoch 52/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2841 - accuracy: 0.8981\n",
      "Epoch 53/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2864 - accuracy: 0.9029\n",
      "Epoch 54/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2875 - accuracy: 0.8981\n",
      "Epoch 55/110\n",
      "20/20 [==============================] - 0s 14ms/step - loss: 0.2894 - accuracy: 0.9029\n",
      "Epoch 56/110\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.2762 - accuracy: 0.9061\n",
      "Epoch 57/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2788 - accuracy: 0.8997\n",
      "Epoch 58/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2782 - accuracy: 0.8997\n",
      "Epoch 59/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2750 - accuracy: 0.9013\n",
      "Epoch 60/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2754 - accuracy: 0.8997\n",
      "Epoch 61/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2701 - accuracy: 0.9076\n",
      "Epoch 62/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2653 - accuracy: 0.9092\n",
      "Epoch 63/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2584 - accuracy: 0.9045\n",
      "Epoch 64/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2705 - accuracy: 0.9045\n",
      "Epoch 65/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2789 - accuracy: 0.8997\n",
      "Epoch 66/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2714 - accuracy: 0.8933\n",
      "Epoch 67/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2549 - accuracy: 0.9140\n",
      "Epoch 68/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2629 - accuracy: 0.8997\n",
      "Epoch 69/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2515 - accuracy: 0.9029\n",
      "Epoch 70/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2518 - accuracy: 0.9108\n",
      "Epoch 71/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2654 - accuracy: 0.9045\n",
      "Epoch 72/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2468 - accuracy: 0.9092\n",
      "Epoch 73/110\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2474 - accuracy: 0.9108\n",
      "Epoch 74/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2470 - accuracy: 0.9140\n",
      "Epoch 75/110\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2451 - accuracy: 0.9108\n",
      "Epoch 76/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2453 - accuracy: 0.9156\n",
      "Epoch 77/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2464 - accuracy: 0.9140\n",
      "Epoch 78/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2437 - accuracy: 0.9156\n",
      "Epoch 79/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2405 - accuracy: 0.9236\n",
      "Epoch 80/110\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2333 - accuracy: 0.9172\n",
      "Epoch 81/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2277 - accuracy: 0.9204\n",
      "Epoch 82/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2295 - accuracy: 0.9172\n",
      "Epoch 83/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2293 - accuracy: 0.9172\n",
      "Epoch 84/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2302 - accuracy: 0.9156\n",
      "Epoch 85/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2327 - accuracy: 0.9108\n",
      "Epoch 86/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2406 - accuracy: 0.9092\n",
      "Epoch 87/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2263 - accuracy: 0.9204\n",
      "Epoch 88/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2360 - accuracy: 0.9140\n",
      "Epoch 89/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2225 - accuracy: 0.9188\n",
      "Epoch 90/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2220 - accuracy: 0.9204\n",
      "Epoch 91/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2203 - accuracy: 0.9188\n",
      "Epoch 92/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2293 - accuracy: 0.9172\n",
      "Epoch 93/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2238 - accuracy: 0.9220\n",
      "Epoch 94/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2219 - accuracy: 0.9172\n",
      "Epoch 95/110\n",
      "20/20 [==============================] - 0s 15ms/step - loss: 0.2183 - accuracy: 0.9188\n",
      "Epoch 96/110\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.2147 - accuracy: 0.9236\n",
      "Epoch 97/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2189 - accuracy: 0.9140\n",
      "Epoch 98/110\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2146 - accuracy: 0.9156\n",
      "Epoch 99/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2138 - accuracy: 0.9252\n",
      "Epoch 100/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2159 - accuracy: 0.9204\n",
      "Epoch 101/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2113 - accuracy: 0.9204\n",
      "Epoch 102/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2153 - accuracy: 0.9124\n",
      "Epoch 103/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2184 - accuracy: 0.9140\n",
      "Epoch 104/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2093 - accuracy: 0.9204\n",
      "Epoch 105/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2117 - accuracy: 0.9252\n",
      "Epoch 106/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2118 - accuracy: 0.9236\n",
      "Epoch 107/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2040 - accuracy: 0.9252\n",
      "Epoch 108/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2051 - accuracy: 0.9172\n",
      "Epoch 109/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2045 - accuracy: 0.9236\n",
      "Epoch 110/110\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2069 - accuracy: 0.9236\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2496 - accuracy: 0.9259\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "Model 5: Validation Accuracy = 0.9259\n",
      "Confusion Matrix:\n",
      "[[ 7 20 29 14 29 20 16]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate each model\n",
    "results = []\n",
    "for i, params in enumerate(hyperparams_list):\n",
    "    result = create_and_evaluate_model(**params)\n",
    "    results.append(result)\n",
    "    print(f\"Model {i+1}: Validation Accuracy = {result['val_accuracy']:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{result['conf_matrix']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "5b195de6-b51f-4573-8300-83c3f7d9b8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HyperParamters for Model  1 :  {'num_of_layers': [64, 32], 'activation': 'relu', 'optimizer': 'adam', 'loss': 'sparse_categorical_crossentropy', 'epochs': 50}\n",
      "Accuracy for Model  1 : 0.9111111164093018\n",
      "Loss for Model  1 : 0.3014119565486908\n",
      "Confusion Matrix for Model  1 : [[ 8 20 29 12 28 21 17]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]]\n",
      "*****************************************************************\n",
      "HyperParamters for Model  2 :  {'num_of_layers': [128, 64, 32], 'activation': 'tanh', 'optimizer': 'sgd', 'loss': 'sparse_categorical_crossentropy', 'epochs': 70}\n",
      "Accuracy for Model  2 : 0.8740741014480591\n",
      "Loss for Model  2 : 0.49106737971305847\n",
      "Confusion Matrix for Model  2 : [[ 7 23 29 13 30 21 12]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]]\n",
      "*****************************************************************\n",
      "HyperParamters for Model  3 :  {'num_of_layers': [256, 128, 64], 'activation': 'relu', 'optimizer': 'adam', 'loss': 'sparse_categorical_crossentropy', 'epochs': 80}\n",
      "Accuracy for Model  3 : 0.9037036895751953\n",
      "Loss for Model  3 : 0.24564462900161743\n",
      "Confusion Matrix for Model  3 : [[ 7 20 28 15 28 21 16]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]]\n",
      "*****************************************************************\n",
      "HyperParamters for Model  4 :  {'num_of_layers': [128, 64], 'activation': 'relu', 'optimizer': 'rmsprop', 'loss': 'sparse_categorical_crossentropy', 'epochs': 90}\n",
      "Accuracy for Model  4 : 0.9407407641410828\n",
      "Loss for Model  4 : 0.2175486981868744\n",
      "Confusion Matrix for Model  4 : [[ 7 16 29 15 29 20 19]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]]\n",
      "*****************************************************************\n",
      "HyperParamters for Model  5 :  {'num_of_layers': [512, 256, 128], 'activation': 'relu', 'optimizer': <keras.src.optimizers.adam.Adam object at 0x00000226A7358490>, 'loss': 'sparse_categorical_crossentropy', 'epochs': 110}\n",
      "Accuracy for Model  5 : 0.9259259104728699\n",
      "Loss for Model  5 : 0.24961912631988525\n",
      "Confusion Matrix for Model  5 : [[ 7 20 29 14 29 20 16]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]]\n",
      "*****************************************************************\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "for result in results:\n",
    "    n=n+1\n",
    "    print(\"HyperParamters for Model \", n , \": \", hyperparams_list[n-1])\n",
    "    print(\"Accuracy for Model \", n, \":\", result['val_accuracy'])\n",
    "    print(\"Loss for Model \", n, \":\", result['loss'])\n",
    "    print(\"Confusion Matrix for Model \", n, \":\", result['conf_matrix'])\n",
    "    print(\"*****************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "145ef9c5-d3a6-4653-85d8-6e2d23940ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accracy of the Best Model:  0.9407407641410828\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Identify the best model\n",
    "best_model_index = np.argmax([res['val_accuracy'] for res in results])\n",
    "best_model = results[best_model_index]['model']\n",
    "print(\"Accracy of the Best Model: \",results[best_model_index]['val_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80416983-c3bd-4ae3-b6be-e6609ea943a2",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "Model 4 demonstrates the highest validation accuracy of  0.9407, which suggests it generalizes better on unseen data compared to the other models.\n",
    "\n",
    "Confusion Matrix Analysis: Model 4 has fewer misclassifications (off-diagonal values) across different classes, indicating balanced performance across both frequent and rare classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a323b0-f840-4969-87b4-1cf4de68f72b",
   "metadata": {},
   "source": [
    "## Evaluate On Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "4f77b1a2-7d67-421b-8c93-4067eafdc09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2205 - accuracy: 0.9185\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "Best Model Test Accuracy: 0.9185\n",
      "Test Set Confusion Matrix:\n",
      "[[ 8  9 37 11 24 31 15]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model on the test set\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=1)\n",
    "y_test_pred = np.argmax(best_model.predict(X_test), axis=-1)\n",
    "y_test_true = np.argmax(y_test, axis=-1)\n",
    "test_conf_matrix = confusion_matrix(y_test_true, y_test_pred)\n",
    "\n",
    "print(f\"Best Model Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Set Confusion Matrix:\\n{test_conf_matrix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ddb3ac-48fd-4e22-a768-8427ca3bc338",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "While the model has an overall high accuracy of 91.85%, it has poor performance in distinguishing between multiple classes, as evidenced by the confusion matrix. For this use case, where accurate classification across multiple categories is likely critical, the model needs further refinement to improve class-specific performance.\n",
    "\n",
    "The loss value of 0.2205 is relatively low, suggesting that the model has a good fit to the training data. However, this doesn't align with the confusion matrix, where misclassification of non-dominant classes is evident.\n",
    "\n",
    "Addressing class imbalance or re-tuning the model to reduce its bias towards the dominant class will be essential for enhancing its ability to make more accurate predictions across all classes.\n",
    "\n",
    "Improving this could involve techniques like resampling the training data, penalizing misclassification of minority classes, or adjusting the class weights to encourage the model to pay more attention to underrepresented classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a83dc8-b265-4c46-8970-33984ff4f4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
